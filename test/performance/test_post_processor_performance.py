"""
–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è PostProcessorPlugin
–ë–µ–Ω—á–º–∞—Ä–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–æ–≤
"""

import pytest
import time
from datetime import datetime
from typing import Dict, List

from src.core.plugin_manager import PluginManager


class TestPostProcessorPerformance:
    """–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ PostProcessorPlugin"""

    @pytest.fixture
    def plugin_manager(self):
        """–§–∏–∫—Å—Ç—É—Ä–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è PluginManager"""
        pm = PluginManager()
        pm.load_plugins()
        pm.setup_plugin_dependencies()
        yield pm
        pm.shutdown_plugins()

    @pytest.fixture
    def small_dataset(self):
        """–ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö —Ç–µ—Å—Ç–æ–≤ (100 –ø–æ—Å—Ç–æ–≤)"""
        return self._generate_posts(100)

    @pytest.fixture
    def medium_dataset(self):
        """–°—Ä–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç (1000 –ø–æ—Å—Ç–æ–≤)"""
        return self._generate_posts(1000)

    @pytest.fixture
    def large_dataset(self):
        """–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç (5000 –ø–æ—Å—Ç–æ–≤)"""
        return self._generate_posts(5000)

    def _generate_posts(self, count: int) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤"""
        posts = []
        keywords_variants = [
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ",
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è",
            "—Ü–∏—Ñ—Ä–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å–∞",
            "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –≤–µ—â–µ–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è",
            "–±–ª–æ–∫—á–µ–π–Ω –∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã",
            "–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∑–∞—â–∏—Ç–∞ –¥–∞–Ω–Ω—ã—Ö",
            "–æ–±–ª–∞—á–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –≤–∏—Ä—Ç—É–∞–ª–∏–∑–∞—Ü–∏—è"
        ]

        for i in range(count):
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (10%)
            if i % 10 == 0 and i > 0:
                link = f"https://vk.com/wall-123_{i-1}"  # –î—É–±–ª–∏–∫–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
            else:
                link = f"https://vk.com/wall-123_{i}"

            text = keywords_variants[i % len(keywords_variants)]
            if i % 7 == 0:  # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Å—Ç—ã –±–µ–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                text = f"–û–±—ã—á–Ω—ã–π –ø–æ—Å—Ç –Ω–æ–º–µ—Ä {i} –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"

            posts.append({
                "link": link,
                "text": text,
                "author": f"Test User {i % 20}",
                "date": f"2025-07-{27 + (i % 3):02d} {10 + (i % 12):02d}:00:00",
                "likes": i % 100,
                "comments": i % 20,
                "reposts": i % 10,
                "views": i * 5
            })

        return posts

    @pytest.mark.benchmark
    def test_standard_processing_performance(self, plugin_manager, medium_dataset, benchmark):
        """–ë–µ–Ω—á–º–∞—Ä–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"]

        result = benchmark(
            post_processor.process_posts,
            medium_dataset,
            keywords,
            False
        )

        assert result["final_count"] > 0
        assert result["processing_time"] > 0
        print(f"\nüìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {result['processing_time']:.3f}—Å ‚Üí {result['final_count']} –ø–æ—Å—Ç–æ–≤")

    @pytest.mark.benchmark
    def test_optimized_processing_performance(self, plugin_manager, medium_dataset, benchmark):
        """–ë–µ–Ω—á–º–∞—Ä–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"]

        result = benchmark(
            post_processor.process_posts_optimized,
            medium_dataset,
            keywords,
            False,
            True,  # early_termination
            True   # lazy_processing
        )

        assert result["final_count"] > 0
        assert result["optimization_level"] == "high"
        print(f"\nüìä –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {result['processing_time']:.3f}—Å ‚Üí {result['final_count']} –ø–æ—Å—Ç–æ–≤")
        print(f"    –õ–µ–Ω–∏–≤—ã—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤: {result.get('lazy_skips', 0)}")

    @pytest.mark.benchmark
    def test_batch_processing_performance(self, plugin_manager, large_dataset, benchmark):
        """–ë–µ–Ω—á–º–∞—Ä–∫ –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"]

        result = benchmark(
            post_processor.process_posts_in_batches,
            large_dataset,
            keywords,
            False,
            500  # batch_size
        )

        assert result["final_count"] > 0
        assert result["optimization_level"] == "batch"
        print(f"\nüìä –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {result['processing_time']:.3f}—Å ‚Üí {result['final_count']} –ø–æ—Å—Ç–æ–≤")
        print(f"    –ë–∞—Ç—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result['batches_processed']}")

    @pytest.mark.benchmark
    def test_cached_processing_performance(self, plugin_manager, medium_dataset, benchmark):
        """–ë–µ–Ω—á–º–∞—Ä–∫ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"]

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∫—ç—à–∞
        post_processor.process_posts_with_cache(medium_dataset, keywords, False, True)

        # –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω —Å –∫—ç—à–µ–º –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞
        result = benchmark(
            post_processor.process_posts_with_cache,
            medium_dataset,
            keywords,
            False,
            True
        )

        assert result["final_count"] > 0
        assert result["optimization_level"] == "cached"
        print(f"\nüìä –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {result['processing_time']:.3f}—Å ‚Üí {result['final_count']} –ø–æ—Å—Ç–æ–≤")
        print(f"    –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∞: {result.get('cache_efficiency', 0):.1f}%")

    @pytest.mark.benchmark
    def test_parallel_processing_performance(self, plugin_manager, large_dataset, benchmark):
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"]

        try:
            result = benchmark(
                post_processor.process_posts_parallel,
                large_dataset,
                keywords,
                False,
                4  # max_workers
            )

            assert result["final_count"] > 0
            assert result["optimization_level"] == "parallel"
            print(f"\nüìä –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {result['processing_time']:.3f}—Å ‚Üí {result['final_count']} –ø–æ—Å—Ç–æ–≤")
            print(f"    –ü–æ—Ç–æ–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {result.get('threads_used', 0)}")

        except ImportError:
            pytest.skip("concurrent.futures –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

    def test_performance_comparison(self, plugin_manager, medium_dataset):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]

        methods = {
            "standard": lambda: post_processor.process_posts(medium_dataset, keywords, False),
            "optimized": lambda: post_processor.process_posts_optimized(medium_dataset, keywords, False),
            "batches": lambda: post_processor.process_posts_in_batches(medium_dataset, keywords, False, 200),
            "cached_first": lambda: post_processor.process_posts_with_cache(medium_dataset, keywords, False, True),
            "cached_second": lambda: post_processor.process_posts_with_cache(medium_dataset, keywords, False, True)
        }

        results = {}

        for method_name, method_func in methods.items():
            start_time = time.time()
            result = method_func()
            end_time = time.time()

            actual_time = end_time - start_time
            reported_time = result.get("processing_time", actual_time)

            results[method_name] = {
                "actual_time": actual_time,
                "reported_time": reported_time,
                "final_count": result["final_count"],
                "optimization_level": result.get("optimization_level", "standard")
            }

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ({len(medium_dataset)} –ø–æ—Å—Ç–æ–≤):")
        print("-" * 70)
        print(f"{'–ú–µ—Ç–æ–¥':<15} {'–í—Ä–µ–º—è (—Å)':<12} {'–†–µ–∑—É–ª—å—Ç–∞—Ç':<10} {'–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è':<15}")
        print("-" * 70)

        for method, data in results.items():
            print(f"{method:<15} {data['actual_time']:<12.3f} {data['final_count']:<10} {data['optimization_level']:<15}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏—è
        standard_time = results["standard"]["actual_time"]
        optimized_time = results["optimized"]["actual_time"]

        if optimized_time < standard_time:
            improvement = ((standard_time - optimized_time) / standard_time) * 100
            print(f"\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ {improvement:.1f}%")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        cached_first = results["cached_first"]["actual_time"]
        cached_second = results["cached_second"]["actual_time"]

        if cached_second < cached_first:
            cache_improvement = ((cached_first - cached_second) / cached_first) * 100
            print(f"üíæ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–∫–æ—Ä–∏–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–∞ {cache_improvement:.1f}%")

    def test_scalability(self, plugin_manager):
        """–¢–µ—Å—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –æ–±—ä—ë–º–∞ –¥–∞–Ω–Ω—ã—Ö"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]

        dataset_sizes = [100, 500, 1000, 2000]
        results = {}

        for size in dataset_sizes:
            dataset = self._generate_posts(size)

            start_time = time.time()
            result = post_processor.process_posts_optimized(dataset, keywords, False)
            end_time = time.time()

            processing_time = end_time - start_time
            throughput = size / processing_time  # –ø–æ—Å—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É

            results[size] = {
                "time": processing_time,
                "throughput": throughput,
                "final_count": result["final_count"]
            }

        print(f"\nüìà –¢–µ—Å—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏:")
        print("-" * 50)
        print(f"{'–†–∞–∑–º–µ—Ä':<10} {'–í—Ä–µ–º—è (—Å)':<12} {'–ü–æ—Å—Ç–æ–≤/—Å':<12} {'–†–µ–∑—É–ª—å—Ç–∞—Ç':<10}")
        print("-" * 50)

        for size, data in results.items():
            print(f"{size:<10} {data['time']:<12.3f} {data['throughput']:<12.1f} {data['final_count']:<10}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–Ω–µ–π–Ω—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
        small_throughput = results[dataset_sizes[0]]["throughput"]
        large_throughput = results[dataset_sizes[-1]]["throughput"]

        # –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–∞ —Å–∏–ª—å–Ω–æ –ø–∞–¥–∞—Ç—å
        throughput_ratio = large_throughput / small_throughput
        assert throughput_ratio > 0.5, f"–°–∏–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {throughput_ratio:.2f}"

        print(f"\nüìä –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏: {throughput_ratio:.2f}")

    def test_memory_efficiency(self, plugin_manager, large_dataset):
        """–¢–µ—Å—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞—Ç—á–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–Ω–æ–π)
        import psutil
        process = psutil.Process()

        memory_before = process.memory_info().rss / 1024 / 1024  # MB

        result = post_processor.process_posts_in_batches(
            large_dataset, keywords, False, batch_size=100
        )

        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = memory_after - memory_before

        print(f"\nüíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:")
        print(f"    –î–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {memory_before:.1f} MB")
        print(f"    –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {memory_after:.1f} MB")
        print(f"    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {memory_used:.1f} MB")
        print(f"    –ü–æ—Å—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(large_dataset)}")
        print(f"    MB –Ω–∞ 1000 –ø–æ—Å—Ç–æ–≤: {(memory_used / len(large_dataset)) * 1000:.2f}")

        # –ü–∞–º—è—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–∞ —Ä–∞—Å—Ç–∏ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ
        memory_per_post = memory_used / len(large_dataset)
        assert memory_per_post < 0.1, f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_per_post:.4f} MB/–ø–æ—Å—Ç"

    def test_cache_efficiency(self, plugin_manager, medium_dataset):
        """–¢–µ—Å—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        post_processor = plugin_manager.get_plugin("post_processor")
        keywords = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]

        # –û—á–∏—â–∞–µ–º –∫—ç—à
        post_processor.clear_cache()

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω - –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—ç—à–∞
        start_time = time.time()
        result1 = post_processor.process_posts_with_cache(medium_dataset, keywords, False, True)
        time1 = time.time() - start_time

        # –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞
        start_time = time.time()
        result2 = post_processor.process_posts_with_cache(medium_dataset, keywords, False, True)
        time2 = time.time() - start_time

        # –¢—Ä–µ—Ç–∏–π –ø—Ä–æ–≥–æ–Ω —Å –Ω–µ–º–Ω–æ–≥–æ –¥—Ä—É–≥–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        start_time = time.time()
        result3 = post_processor.process_posts_with_cache(medium_dataset, ["—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"], False, True)
        time3 = time.time() - start_time

        print(f"\nüíæ –¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è:")
        print(f"    –ü–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω (–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ): {time1:.3f}—Å")
        print(f"    –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω (–∫—ç—à): {time2:.3f}—Å")
        print(f"    –¢—Ä–µ—Ç–∏–π –ø—Ä–æ–≥–æ–Ω (—á–∞—Å—Ç–∏—á–Ω—ã–π –∫—ç—à): {time3:.3f}—Å")

        # –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±—ã—Å—Ç—Ä–µ–µ
        if time2 < time1:
            improvement = ((time1 - time2) / time1) * 100
            print(f"    –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∫—ç—à–∞: {improvement:.1f}%")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
        cache_stats = post_processor.get_cache_stats()
        print(f"    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞: {cache_stats}")

        # –ö—ç—à –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
        if cache_stats["cache_enabled"]:
            assert cache_stats["total_memory_items"] > 0

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏
        assert result1["final_count"] == result2["final_count"]


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é
    test_class = TestPostProcessorPerformance()

    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    pm = PluginManager()
    pm.load_plugins()
    pm.setup_plugin_dependencies()

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    small_data = test_class._generate_posts(100)
    medium_data = test_class._generate_posts(1000)

    try:
        print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
        test_class.test_performance_comparison(pm, medium_data)

        # –¢–µ—Å—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏
        test_class.test_scalability(pm)

        print("\nüéâ –¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–∞—Ö: {e}")
        import traceback
        traceback.print_exc()
    finally:
        pm.shutdown_plugins()
