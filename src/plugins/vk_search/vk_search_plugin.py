"""
–ü–ª–∞–≥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ VK API
"""

import asyncio
import time
from typing import Any, Dict, List

import aiohttp

from src.core.event_system import EventType
from src.plugins.base_plugin import BasePlugin
from src.plugins.vk_search.vk_time_utils import to_vk_timestamp


class VKSearchPlugin(BasePlugin):
    """–ü–ª–∞–≥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ VK API"""

    def __init__(self):
        super().__init__()
        self.name = "VKSearchPlugin"
        self.version = "1.0.0"
        self.description = "–ü–ª–∞–≥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Å—Ç–æ–≤ –≤ VK —á–µ—Ä–µ–∑ API"

        # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø–ª–∞–≥–∏–Ω–æ–≤
        self.token_manager = None

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        self.config = {
            "access_token": None,
            "api_version": "5.131",
            "request_delay": 0.05,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 0.1 –¥–æ 0.05 (–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ)
            "max_requests_per_second": 12,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 8 –¥–æ 12
            "timeout": 10,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 15 –¥–æ 10
            "max_retries": 3,
            "batch_size": 12,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 8 –¥–æ 12
            "max_batches": 15,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 10 –¥–æ 15
            "use_connection_pooling": True,
            "enable_caching": True,
            "cache_ttl": 600,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 10 –º–∏–Ω—É—Ç
            "adaptive_rate_limiting": True,
            "min_delay": 0.03,  # –ï—â–µ –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            "max_delay": 0.8,  # –£–º–µ–Ω—å—à–µ–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.requests_made = 0
        self.session = None
        self.cache = {}
        self.token_usage = {}
        self.rate_limit_hits = 0
        self.response_times = []
        self.last_request_time = 0

        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "popular_queries": {},
            "query_patterns": {},
            "cache_size_limit": 1000,
            "preload_enabled": True,
        }

        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.popular_queries = [
            "–Ω–æ–≤–æ—Å—Ç–∏",
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
            "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "python",
            "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞",
            "web",
            "mobile",
            "ai",
            "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "data science",
        ]

    def initialize(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        self.log_info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞ VK Search")

        if not self.validate_config():
            self.log_error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞")
            return

        self.log_info("–ü–ª–∞–≥–∏–Ω VK Search –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.emit_event(EventType.PLUGIN_LOADED, {"status": "initialized"})

    def set_token_manager(self, token_manager):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–≤—è–∑—å —Å TokenManagerPlugin"""
        self.token_manager = token_manager
        self.log_info("TokenManager –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ VKSearchPlugin")

    def shutdown(self) -> None:
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–ª–∞–≥–∏–Ω–∞"""
        self.log_info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–ª–∞–≥–∏–Ω–∞ VK Search")

        if self.session:
            asyncio.create_task(self.session.close())

        self.emit_event(EventType.PLUGIN_UNLOADED, {"status": "shutdown"})
        self.log_info("–ü–ª–∞–≥–∏–Ω VK Search –∑–∞–≤–µ—Ä—à–µ–Ω")

    def validate_config(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if not self.config.get("access_token"):
            self.log_error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç access_token")
            return False
        return True

    def get_required_config_keys(self) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return ["access_token"]

    async def _rate_limit(self) -> None:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π Rate limiting –¥–ª—è VK API"""
        current_time = time.time()

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–µ—Ä–∂–∫–∏
        if self.config["adaptive_rate_limiting"]:
            # –ï—Å–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ –±—ã–ª rate limit, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
            if self.rate_limit_hits > 0:
                delay = min(self.config["max_delay"], self.config["request_delay"] * (1.5**self.rate_limit_hits))
            else:
                # –ï—Å–ª–∏ –≤—Å–µ —Ö–æ—Ä–æ—à–æ, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                delay = max(self.config["min_delay"], self.config["request_delay"] * 0.95)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –≤ –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
            delay = max(self.config["min_delay"], min(self.config["max_delay"], delay))
        else:
            delay = self.config["request_delay"]

        if delay > 0:
            await asyncio.sleep(delay)

        self.last_request_time = current_time

    def _get_best_token(self, available_tokens: List[str]) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Ç–æ–∫–µ–Ω —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π"""
        if not self.token_usage:
            return available_tokens[0]

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–∫–µ–Ω —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
        best_token = min(self.token_usage.items(), key=lambda x: x[1])[0]

        # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–µ –≤ —Å–ø–∏—Å–∫–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π
        if best_token not in available_tokens:
            return available_tokens[0]

        return best_token

    def _update_token_usage(self, token: str):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞"""
        self.token_usage[token] = self.token_usage.get(token, 0) + 1

    def _is_cache_valid(self, cache_entry: dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫—ç—à–∞"""
        if not self.config["enable_caching"]:
            return False

        current_time = time.time()
        return current_time - cache_entry["timestamp"] < self.config["cache_ttl"]

    def _update_cache_stats(self, cache_key: str, hit: bool):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        if hit:
            self.cache_stats["hits"] += 1
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
            self.cache_stats["popular_queries"][cache_key] = self.cache_stats["popular_queries"].get(cache_key, 0) + 1
        else:
            self.cache_stats["misses"] += 1

    def _analyze_query_patterns(self, query: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–ø—Ä–æ—Å–æ–≤"""
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑: —Ä–∞–∑–±–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–ª–æ–≤–∞
        words = query.lower().split()
        for word in words:
            if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                self.cache_stats["query_patterns"][word] = self.cache_stats["query_patterns"].get(word, 0) + 1

    def _preload_popular_queries(self):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã"""
        if not self.config["enable_caching"] or not self.cache_stats["preload_enabled"]:
            return

        self.log_info("üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        popular = sorted(self.cache_stats["popular_queries"].items(), key=lambda x: x[1], reverse=True)[:5]

        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        for query in self.popular_queries:
            if query not in [p[0] for p in popular]:
                popular.append((query, 1))

        self.log_info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(popular)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏")

    def _smart_cache_cleanup(self):
        """–£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if len(self.cache) <= self.cache_stats["cache_size_limit"]:
            return

        self.log_info("üßπ –£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞...")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫—ç—à –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏
        cache_items = []
        for key, entry in self.cache.items():
            popularity = self.cache_stats["popular_queries"].get(key, 0)
            age = time.time() - entry["timestamp"]
            score = popularity - (age / 3600)  # –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –º–∏–Ω—É—Å –≤–æ–∑—Ä–∞—Å—Ç –≤ —á–∞—Å–∞—Ö
            cache_items.append((key, score))

        # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∏ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
        cache_items.sort(key=lambda x: x[1])
        to_remove = len(self.cache) - self.cache_stats["cache_size_limit"] // 2

        for key, _ in cache_items[:to_remove]:
            del self.cache[key]

        self.log_info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {to_remove} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")

    def _get_cache_key(self, params: dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–∞"""
        import hashlib

        key_data = str(sorted(params.items()))
        return hashlib.md5(key_data.encode(), usedforsecurity=False).hexdigest()

    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–ª–∞–≥–∏–Ω–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        avg_response_time = 0
        if self.response_times:
            avg_response_time = sum(self.response_times) / len(self.response_times)

        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫—ç—à–∞
        total_cache_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        cache_hit_rate = 0
        if total_cache_requests > 0:
            cache_hit_rate = self.cache_stats["hits"] / total_cache_requests

        # –¢–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        top_queries = sorted(self.cache_stats["popular_queries"].items(), key=lambda x: x[1], reverse=True)[:5]

        # –¢–æ–ø –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
        top_patterns = sorted(self.cache_stats["query_patterns"].items(), key=lambda x: x[1], reverse=True)[:5]

        return {
            "requests_made": self.requests_made,
            "enabled": self.is_enabled(),
            "config": self.get_config(),
            "performance_metrics": {
                "average_response_time": round(avg_response_time, 3),
                "rate_limit_hits": self.rate_limit_hits,
                "cache_size": len(self.cache),
                "cache_hit_rate": round(cache_hit_rate, 3),
                "token_usage": self.token_usage,
                "requests_per_second": (
                    round(self.requests_made / max(1, avg_response_time), 2) if avg_response_time > 0 else 0
                ),
            },
            "intelligent_caching": {
                "cache_hits": self.cache_stats["hits"],
                "cache_misses": self.cache_stats["misses"],
                "total_cache_requests": total_cache_requests,
                "cache_hit_rate": round(cache_hit_rate, 3),
                "top_popular_queries": top_queries,
                "top_query_patterns": top_patterns,
                "cache_size_limit": self.cache_stats["cache_size_limit"],
                "preload_enabled": self.cache_stats["preload_enabled"],
            },
        }

    async def search_multiple_queries(
        self,
        queries: List[str],
        start_date,
        end_date,
        exact_match: bool = True,
        minus_words: List[str] = None,
        batch_size: int = 3,
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º
        """
        self.log_info(f"üîç –ü–æ–∏—Å–∫ –ø–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–∞–º")

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä—ã (–∑–∞–ø—Ä–æ—Å, —Ç–æ–∫–µ–Ω)
        keyword_token_pairs = [(query, self.config["access_token"]) for query in queries]

        # –í—ã–∑—ã–≤–∞–µ–º –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫
        return await self.mass_search_with_tokens(
            keyword_token_pairs, start_date, end_date, exact_match, minus_words, batch_size
        )

    async def _search_single_query(
        self, session, query: str, start_date, end_date, exact_match: bool, minus_words: List[str] = None
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        """
        params = {
            "q": f'"{query}"' if exact_match else query,
            "count": 200,
            "extended": 1,
            "access_token": self.config["access_token"],
            "v": self.config["api_version"],
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞—Ç—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if start_date is not None:
            params["start_time"] = start_date
        if end_date is not None:
            params["end_time"] = end_date

        # –î–æ–±–∞–≤–ª—è–µ–º –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞
        if minus_words:
            for word in minus_words:
                if word.strip():
                    params["q"] += f" -{word.strip()}"

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        posts = await self._fetch_vk_batch(session, params, query)
        return posts

    def _check_cache_for_request(self, params, query):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫—ç—à –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        cache_key = self._get_cache_key(params)

        if cache_key in self.cache and self._is_cache_valid(self.cache[cache_key]):
            # –õ–æ–≥–∏—Ä—É–µ–º cache hit —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
            if self.requests_made < 50:
                self.log_info(f"üìã –ö—ç—à-—Ö–∏—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
            self._update_cache_stats(cache_key, True)
            return self.cache[cache_key]["data"]

        self._update_cache_stats(cache_key, False)
        return None

    async def _handle_vk_api_response(self, response, query, start_time, params, cache_key):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç VK API"""
        import time

        if response.status != 200:
            if self.requests_made < 50:
                self.log_error(f"HTTP –æ—à–∏–±–∫–∞ {response.status} –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
            return []

        data = await response.json()

        if "error" in data:
            return await self._handle_api_error(data["error"], query)

        if "response" not in data:
            if self.requests_made < 50:
                self.log_error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç VK API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {data}")
            return []

        items = data["response"].get("items", [])

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        self._cache_response(cache_key, items, query, params)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
        response_time = time.time() - start_time
        self.response_times.append(response_time)

        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ rate limit –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω
        if self.rate_limit_hits > 0:
            self.rate_limit_hits = max(0, self.rate_limit_hits - 1)

        return items

    async def _handle_api_error(self, error, query):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ VK API"""
        error_code = error.get("error_code")

        if error_code == 6:  # Too many requests per second
            if self.rate_limit_hits < 10:
                self.log_warning(f"Rate limit –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}', –æ–∂–∏–¥–∞–Ω–∏–µ...")
            self.rate_limit_hits += 1
            await asyncio.sleep(1)
            return "retry"  # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–¥ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–∞
        else:
            if self.requests_made < 100:
                self.log_error(f"–û—à–∏–±–∫–∞ VK API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {error}")
            return []

    def _cache_response(self, cache_key, items, query, params):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –∫—ç—à"""
        import time

        if self.config["enable_caching"]:
            self.cache[cache_key] = {
                "data": items,
                "timestamp": time.time(),
                "query": query,
                "params": params,
            }
            # –£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            self._smart_cache_cleanup()

    async def _make_vk_request(self, session, params, query, attempt):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ VK API"""
        import time

        await self._rate_limit()

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞
        token = params.get("access_token")
        if token:
            self._update_token_usage(token)

        start_time = time.time()
        cache_key = self._get_cache_key(params)

        try:
            async with session.get("https://api.vk.com/method/newsfeed.search", params=params) as response:
                self.requests_made += 1
                return await self._handle_vk_api_response(response, query, start_time, params, cache_key)

        except Exception as e:
            if self.requests_made < 100:
                self.log_error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è '{query}': {e}")
            return []

    async def _fetch_vk_batch(self, session, params, query, retry_count=3):
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –ø–∞—Ä—Ç–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç VK API —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–ø—Ä–æ—Å–∞
        self._analyze_query_patterns(query)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_result = self._check_cache_for_request(params, query)
        if cached_result is not None:
            return cached_result

        # –ü–æ–ø—ã—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å retry –ª–æ–≥–∏–∫–æ–π
        for attempt in range(retry_count):
            result = await self._make_vk_request(session, params, query, attempt)

            if result == "retry":
                continue  # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å –ø—Ä–∏ rate limit
            elif isinstance(result, list):
                return result  # –£—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

            # –ü—Ä–∏ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–∫–∞—Ö –∂–¥—ë–º –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
            if attempt < retry_count - 1:
                await asyncio.sleep(1)

        return []  # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã

    def _parse_datetime(self, datetime_str: str) -> int:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –≤ timestamp —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º vk_time_utils
        """
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –Ω–∞ –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
            date_part, time_part = datetime_str.split(" ")
            return to_vk_timestamp(date_part, time_part, "Europe/Moscow")
        except ValueError:
            raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {datetime_str}. –û–∂–∏–¥–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç: DD.MM.YYYY HH:MM")

    def _convert_dates_to_timestamps(self, start_date, end_date):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç—ã –≤ timestamp –¥–ª—è VK API"""
        _start_ts = start_date
        _end_ts = end_date

        if isinstance(start_date, str):
            date_part, time_part = start_date.split(" ")
            _start_ts = to_vk_timestamp(date_part, time_part, "Europe/Moscow")

        if isinstance(end_date, str):
            date_part, time_part = end_date.split(" ")
            _end_ts = to_vk_timestamp(date_part, time_part, "Europe/Moscow")

        return _start_ts, _end_ts

    def _log_search_progress(self, total_queries, batch_size, current_index=None, progress_type="start"):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–∏—Å–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤"""
        if progress_type == "start":
            if total_queries > 10:
                self.log_info(f"üöÄ –ú–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫: {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤, batch_size={batch_size}")
            else:
                self.log_info(f"üöÄ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤")
                self.log_info(f"‚öôÔ∏è Batch size: {batch_size}, Max batches: {self.config['max_batches']}")

        elif progress_type == "batch" and current_index is not None:
            if total_queries > 20 and current_index % (batch_size * 5) == 0:
                progress = (current_index / total_queries) * 100
                self.log_info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({current_index}/{total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤)")

        elif progress_type == "batch_result":
            if total_queries > 20:
                batch_num = (current_index // batch_size) + 1
                self.log_info(f"üìà –ë–∞—Ç—á {batch_num}: –ø–æ–ª—É—á–µ–Ω–æ –ø–æ—Å—Ç–æ–≤")

    def _create_search_params(self, keyword, token, exact_match, start_ts, end_ts, minus_words):
        """–°–æ–∑–¥–∞—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        params = {
            "q": f'"{keyword}"' if exact_match else keyword,
            "count": 200,
            "extended": 1,
            "access_token": token,
            "v": self.config["api_version"],
        }

        if start_ts is not None:
            params["start_time"] = start_ts
        if end_ts is not None:
            params["end_time"] = end_ts

        if minus_words:
            for word in minus_words:
                if word.strip():
                    params["q"] += f" -{word.strip()}"

        return params

    async def _process_search_batch(self, session, batch, exact_match, start_ts, end_ts, minus_words):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –±–∞—Ç—á –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        tasks = []

        for keyword, token in batch:
            # –£–º–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ - –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π
            available_tokens = [t for _, t in batch]
            best_token = self._get_best_token(available_tokens)

            params = self._create_search_params(keyword, best_token, exact_match, start_ts, end_ts, minus_words)

            # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö offset'–æ–≤
            max_batches = self.config["max_batches"]
            offsets = [j * 200 for j in range(max_batches)]

            for offset in offsets:
                params_copy = params.copy()
                params_copy["offset"] = offset
                tasks.append(self._fetch_vk_batch(session, params_copy, keyword))

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        batch_posts = []

        for result in results:
            if isinstance(result, list):
                batch_posts.extend(result)
            elif isinstance(result, Exception):
                self._handle_search_error(result)

        return batch_posts

    def _handle_search_error(self, error):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –ø–æ–∏—Å–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –æ—à–∏–±–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
        if self.requests_made < 50:
            self.log_error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {error}")

    def _log_final_statistics(self, total_queries, total_posts):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∏—Å–∫–∞"""
        if total_queries > 10:
            self.log_info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {total_posts} –ø–æ—Å—Ç–æ–≤, {self.requests_made} –∑–∞–ø—Ä–æ—Å–æ–≤")
        else:
            self.log_info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {total_posts} –ø–æ—Å—Ç–æ–≤ –æ—Ç VK API")
            self.log_info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.requests_made} –∑–∞–ø—Ä–æ—Å–æ–≤, {len(self.response_times)} –∏–∑–º–µ—Ä–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–∏")

    async def mass_search_with_tokens(
        self,
        keyword_token_pairs: List[tuple] = None,
        queries: List[str] = None,
        tokens: List[str] = None,
        start_date=None,
        end_date=None,
        exact_match: bool = True,
        minus_words: List[str] = None,
        batch_size: int = None,
    ) -> List[Dict[str, Any]]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É–º–Ω–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π —Ç–æ–∫–µ–Ω–æ–≤

        Args:
            keyword_token_pairs: –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - –ø–∞—Ä—ã (keyword, token)
            queries: –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç - —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            tokens: –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç - —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–±–æ–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤—ã–∑–æ–≤–∞
        if keyword_token_pairs:
            # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
            pairs = keyword_token_pairs
        elif queries and tokens:
            # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç - —Å–æ–∑–¥–∞—ë–º –ø–∞—Ä—ã —Å —Ä–æ—Ç–∞—Ü–∏–µ–π —Ç–æ–∫–µ–Ω–æ–≤
            pairs = []
            for i, query in enumerate(queries):
                token = tokens[i % len(tokens)]
                pairs.append((query, token))
        else:
            raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å –ª–∏–±–æ keyword_token_pairs, –ª–∏–±–æ queries+tokens")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π batch_size –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if batch_size is None:
            batch_size = self.config["batch_size"]

        total_queries = len(pairs)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞—Ç
        start_ts, end_ts = self._convert_dates_to_timestamps(start_date, end_date)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –ø–æ–∏—Å–∫–∞
        self._log_search_progress(total_queries, batch_size, progress_type="start")

        all_posts = []

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP –∫–ª–∏–µ–Ω—Ç–∞
        timeout = aiohttp.ClientTimeout(total=self.config["timeout"])
        connector = (
            aiohttp.TCPConnector(
                limit=150,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100 –¥–æ 150
                limit_per_host=30,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 20 –¥–æ 30
                ttl_dns_cache=600,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 10 –º–∏–Ω—É—Ç
                use_dns_cache=True,
                keepalive_timeout=30,  # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
                enable_cleanup_closed=True  # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
            )
        )

        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–∞—Ç—á–∞–º–∏
            for i in range(0, len(pairs), batch_size):
                batch = pairs[i : i + batch_size]

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                self._log_search_progress(total_queries, batch_size, i, "batch")

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
                batch_posts = await self._process_search_batch(
                    session, batch, exact_match, start_ts, end_ts, minus_words
                )
                all_posts.extend(batch_posts)

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –±–∞—Ç—á–∞
                if total_queries > 20:
                    self._log_search_progress(total_queries, batch_size, i, "batch_result")

        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._cleanup_cache()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç—Ä–æ–≥—É—é –ª–æ–∫–∞–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        if all_posts:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ –≤—ã–∑–æ–≤–∞
            filter_keywords = []
            if queries:
                filter_keywords = queries
            elif keyword_token_pairs:
                filter_keywords = [pair[0] for pair in keyword_token_pairs]  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞

            if filter_keywords:
                all_posts = self._strict_local_filter(all_posts, filter_keywords, exact_match)

        self._log_final_statistics(total_queries, len(all_posts))

        return all_posts

    def _cleanup_cache(self):
        """–û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞"""
        if not self.config["enable_caching"]:
            return

        expired_keys = []

        for key, entry in self.cache.items():
            if not self._is_cache_valid(entry):
                expired_keys.append(key)

        for key in expired_keys:
            del self.cache[key]

        if expired_keys:
            self.log_info(f"üßπ –û—á–∏—â–µ–Ω–æ {len(expired_keys)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞")

    def _strict_local_filter(self, posts: List[Dict], keywords: List[str], exact_match: bool = True) -> List[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–≥–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã VK API, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        if not posts or not keywords:
            return posts

        filtered_posts = []

        for post in posts:
            text = post.get('text', '').strip() if post.get('text') else ''
            if not text:
                continue

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ - —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            normalized_text = self._normalize_text_for_search(text)
            matched_keywords = []

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            for keyword in keywords:
                keyword_normalized = self._normalize_text_for_search(keyword)

                if exact_match:
                    # –ë–æ–ª–µ–µ –≥–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏—â–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫—É
                    if keyword_normalized.lower() in normalized_text.lower():
                        matched_keywords.append(keyword)
                else:
                    # –ú–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –≤—Å–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
                    words = keyword_normalized.lower().split()
                    if all(word in normalized_text.lower() for word in words):
                        matched_keywords.append(keyword)

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if matched_keywords:
                post['keywords_matched'] = matched_keywords
                filtered_posts.append(post)

        original_count = len(posts)
        filtered_count = len(filtered_posts)
        self.log_info(f"üîç –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {original_count} ‚Üí {filtered_count} –ø–æ—Å—Ç–æ–≤")

        return filtered_posts

    def _normalize_text_for_search(self, text: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        –£–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Å–º—ã—Å–ª
        """
        if not text:
            return ""

        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –∏ –ø—Ä–æ–±–µ–ª—ã
        normalized = ' '.join(text.split())

        # –£–±–∏—Ä–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        import re
        normalized = re.sub(r'[^\w\s\.\,\!\?\-\‚Äî\¬´\¬ª\"\'\:\;\(\)]', ' ', normalized)

        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = re.sub(r'\s+', ' ', normalized)

        return normalized.strip()

    def _preload_cache_for_keywords(self, keywords: List[str]) -> None:
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        """
        preload_keywords = []
        for keyword in keywords:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∫—ç—à–µ
            similar_keys = [k for k in self.cache.keys() if keyword.lower() in k.lower()]
            if not similar_keys:
                preload_keywords.append(keyword)

        if preload_keywords:
            self.log_info(f"üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –¥–ª—è {len(preload_keywords)} –Ω–æ–≤—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")

    def _get_cache_suggestions(self, query: str) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞ –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        suggestions = []
        query_lower = query.lower()

        for cached_key in self.cache.keys():
            if query_lower in cached_key.lower() or cached_key.lower() in query_lower:
                suggestions.append(cached_key)

        return suggestions[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

    def _optimize_query_order(self, keyword_token_pairs: List[tuple]) -> List[tuple]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ä—è–¥–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤: —Å–Ω–∞—á–∞–ª–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –ø–æ—Ç–æ–º –Ω–æ–≤—ã–µ
        """
        cached_pairs = []
        new_pairs = []

        for pair in keyword_token_pairs:
            query_key = self._get_cache_key({"q": pair[0], "token": pair[1]})
            if query_key in self.cache:
                cached_pairs.append(pair)
            else:
                new_pairs.append(pair)

        # –°–Ω–∞—á–∞–ª–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ (–±—ã—Å—Ç—Ä—ã–µ), –ø–æ—Ç–æ–º –Ω–æ–≤—ã–µ
        optimized_order = cached_pairs + new_pairs

        if len(cached_pairs) > 0:
            self.log_info(f"üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: {len(cached_pairs)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫—ç—à–∞, {len(new_pairs)} –Ω–æ–≤—ã—Ö")

        return optimized_order
