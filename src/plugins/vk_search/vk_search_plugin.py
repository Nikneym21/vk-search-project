"""
–ü–ª–∞–≥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ VK API
"""

import asyncio
import aiohttp
import time
from typing import Dict, List, Any, Optional
from datetime import datetime

from ...core.event_system import EventType
from ..base_plugin import BasePlugin


class VKSearchPlugin(BasePlugin):
    """–ü–ª–∞–≥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ VK API"""
    
    def __init__(self):
        super().__init__()
        self.name = "VKSearchPlugin"
        self.version = "1.0.0"
        self.description = "–ü–ª–∞–≥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Å—Ç–æ–≤ –≤ VK —á–µ—Ä–µ–∑ API"
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        self.config = {
            "access_token": None,
            "api_version": "5.131",
            "request_delay": 0.1,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 0.3 –¥–æ 0.1
            "max_requests_per_second": 8,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 8
            "timeout": 15,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 30 –¥–æ 15
            "max_retries": 3,
            "batch_size": 8,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 8
            "max_batches": 10,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–æ 10
            "use_connection_pooling": True,
            "enable_caching": True,
            "cache_ttl": 300,  # 5 –º–∏–Ω—É—Ç
            "adaptive_rate_limiting": True,
            "min_delay": 0.05,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            "max_delay": 1.0,   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.requests_made = 0
        self.session = None
        self.cache = {}
        self.token_usage = {}
        self.rate_limit_hits = 0
        self.response_times = []
        self.last_request_time = 0
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "popular_queries": {},
            "query_patterns": {},
            "cache_size_limit": 1000,
            "preload_enabled": True
        }
        
        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.popular_queries = [
            "–Ω–æ–≤–æ—Å—Ç–∏", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "python", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞",
            "web", "mobile", "ai", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "data science"
        ]
    
    def initialize(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        self.log_info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞ VK Search")
        
        if not self.validate_config():
            self.log_error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞")
            return
        
        self.log_info("–ü–ª–∞–≥–∏–Ω VK Search –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.emit_event(EventType.PLUGIN_LOADED, {"status": "initialized"})
    
    def shutdown(self) -> None:
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–ª–∞–≥–∏–Ω–∞"""
        self.log_info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–ª–∞–≥–∏–Ω–∞ VK Search")
        
        if self.session:
            asyncio.create_task(self.session.close())
        
        self.emit_event(EventType.PLUGIN_UNLOADED, {"status": "shutdown"})
        self.log_info("–ü–ª–∞–≥–∏–Ω VK Search –∑–∞–≤–µ—Ä—à–µ–Ω")
    
    def validate_config(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if not self.config.get("access_token"):
            self.log_error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç access_token")
            return False
        return True
    
    def get_required_config_keys(self) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return ["access_token"]
    
    async def _rate_limit(self) -> None:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π Rate limiting –¥–ª—è VK API"""
        current_time = time.time()
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–µ—Ä–∂–∫–∏
        if self.config["adaptive_rate_limiting"]:
            # –ï—Å–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ –±—ã–ª rate limit, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
            if self.rate_limit_hits > 0:
                delay = min(self.config["max_delay"], 
                           self.config["request_delay"] * (1.5 ** self.rate_limit_hits))
            else:
                # –ï—Å–ª–∏ –≤—Å–µ —Ö–æ—Ä–æ—à–æ, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                delay = max(self.config["min_delay"], 
                           self.config["request_delay"] * 0.95)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –≤ –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
            delay = max(self.config["min_delay"], 
                       min(self.config["max_delay"], delay))
        else:
            delay = self.config["request_delay"]
        
        if delay > 0:
            await asyncio.sleep(delay)
        
        self.last_request_time = current_time
    
    def _get_best_token(self, available_tokens: List[str]) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Ç–æ–∫–µ–Ω —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π"""
        if not self.token_usage:
            return available_tokens[0]
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–∫–µ–Ω —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
        best_token = min(self.token_usage.items(), key=lambda x: x[1])[0]
        
        # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–µ –≤ —Å–ø–∏—Å–∫–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π
        if best_token not in available_tokens:
            return available_tokens[0]
        
        return best_token
    
    def _update_token_usage(self, token: str):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞"""
        self.token_usage[token] = self.token_usage.get(token, 0) + 1
    
    def _is_cache_valid(self, cache_entry: dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫—ç—à–∞"""
        if not self.config["enable_caching"]:
            return False
        
        current_time = time.time()
        return current_time - cache_entry["timestamp"] < self.config["cache_ttl"]
    
    def _update_cache_stats(self, cache_key: str, hit: bool):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        if hit:
            self.cache_stats["hits"] += 1
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
            self.cache_stats["popular_queries"][cache_key] = \
                self.cache_stats["popular_queries"].get(cache_key, 0) + 1
        else:
            self.cache_stats["misses"] += 1
    
    def _analyze_query_patterns(self, query: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–ø—Ä–æ—Å–æ–≤"""
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑: —Ä–∞–∑–±–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–ª–æ–≤–∞
        words = query.lower().split()
        for word in words:
            if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                self.cache_stats["query_patterns"][word] = \
                    self.cache_stats["query_patterns"].get(word, 0) + 1
    
    def _preload_popular_queries(self):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã"""
        if not self.config["enable_caching"] or not self.cache_stats["preload_enabled"]:
            return
        
        self.log_info("üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        popular = sorted(
            self.cache_stats["popular_queries"].items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:5]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        for query in self.popular_queries:
            if query not in [p[0] for p in popular]:
                popular.append((query, 1))
        
        self.log_info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(popular)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏")
    
    def _smart_cache_cleanup(self):
        """–£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if len(self.cache) <= self.cache_stats["cache_size_limit"]:
            return
        
        self.log_info("üßπ –£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞...")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫—ç—à –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏
        cache_items = []
        for key, entry in self.cache.items():
            popularity = self.cache_stats["popular_queries"].get(key, 0)
            age = time.time() - entry["timestamp"]
            score = popularity - (age / 3600)  # –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –º–∏–Ω—É—Å –≤–æ–∑—Ä–∞—Å—Ç –≤ —á–∞—Å–∞—Ö
            cache_items.append((key, score))
        
        # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∏ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
        cache_items.sort(key=lambda x: x[1])
        to_remove = len(self.cache) - self.cache_stats["cache_size_limit"] // 2
        
        for key, _ in cache_items[:to_remove]:
            del self.cache[key]
        
        self.log_info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {to_remove} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
    
    def _get_cache_key(self, params: dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–∞"""
        import hashlib
        key_data = str(sorted(params.items()))
        return hashlib.md5(key_data.encode()).hexdigest()

    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–ª–∞–≥–∏–Ω–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        avg_response_time = 0
        if self.response_times:
            avg_response_time = sum(self.response_times) / len(self.response_times)
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫—ç—à–∞
        total_cache_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        cache_hit_rate = 0
        if total_cache_requests > 0:
            cache_hit_rate = self.cache_stats["hits"] / total_cache_requests
        
        # –¢–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        top_queries = sorted(
            self.cache_stats["popular_queries"].items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:5]
        
        # –¢–æ–ø –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
        top_patterns = sorted(
            self.cache_stats["query_patterns"].items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:5]
        
        return {
            "requests_made": self.requests_made,
            "enabled": self.is_enabled(),
            "config": self.get_config(),
            "performance_metrics": {
                "average_response_time": round(avg_response_time, 3),
                "rate_limit_hits": self.rate_limit_hits,
                "cache_size": len(self.cache),
                "cache_hit_rate": round(cache_hit_rate, 3),
                "token_usage": self.token_usage,
                "requests_per_second": round(self.requests_made / max(1, avg_response_time), 2) if avg_response_time > 0 else 0
            },
            "intelligent_caching": {
                "cache_hits": self.cache_stats["hits"],
                "cache_misses": self.cache_stats["misses"],
                "total_cache_requests": total_cache_requests,
                "cache_hit_rate": round(cache_hit_rate, 3),
                "top_popular_queries": top_queries,
                "top_query_patterns": top_patterns,
                "cache_size_limit": self.cache_stats["cache_size_limit"],
                "preload_enabled": self.cache_stats["preload_enabled"]
            }
        }

    async def search_multiple_queries(self, queries: List[str], start_date, end_date, 
                                    exact_match: bool = True, minus_words: List[str] = None, batch_size: int = 3) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º
        """
        self.log_info(f"üîç –ü–æ–∏—Å–∫ –ø–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–∞–º")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä—ã (–∑–∞–ø—Ä–æ—Å, —Ç–æ–∫–µ–Ω)
        keyword_token_pairs = [(query, self.config["access_token"]) for query in queries]
        
        # –í—ã–∑—ã–≤–∞–µ–º –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫
        return await self.mass_search_with_tokens(
            keyword_token_pairs, start_date, end_date, exact_match, minus_words, batch_size
        )

    async def _search_single_query(self, session, query: str, start_date, end_date, 
                                  exact_match: bool, minus_words: List[str] = None) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        """
        params = {
            'q': f'"{query}"' if exact_match else query,
            'count': 200,
            'extended': 1,
            'access_token': self.config["access_token"],
            'v': self.config["api_version"]
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞—Ç—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if start_date is not None:
            params['start_time'] = start_date
        if end_date is not None:
            params['end_time'] = end_date
            
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞
        if minus_words:
            for word in minus_words:
                if word.strip():
                    params['q'] += f' -{word.strip()}'
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        posts = await self._fetch_vk_batch(session, params, query)
        return posts

    async def _fetch_vk_batch(self, session, params, query, retry_count=3):
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –ø–∞—Ä—Ç–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç VK API —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        import time
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–ø—Ä–æ—Å–∞
        self._analyze_query_patterns(query)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = self._get_cache_key(params)
        if cache_key in self.cache and self._is_cache_valid(self.cache[cache_key]):
            # –õ–æ–≥–∏—Ä—É–µ–º cache hit —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
            if self.requests_made < 50:
                self.log_info(f"üìã –ö—ç—à-—Ö–∏—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
            self._update_cache_stats(cache_key, True)
            return self.cache[cache_key]["data"]
        
        self._update_cache_stats(cache_key, False)
        start_time = time.time()
        
        for attempt in range(retry_count):
            try:
                await self._rate_limit()
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞
                token = params.get('access_token')
                if token:
                    self._update_token_usage(token)
                
                async with session.get('https://api.vk.com/method/newsfeed.search', params=params) as response:
                    self.requests_made += 1
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        if 'error' in data:
                            error_code = data['error'].get('error_code')
                            if error_code == 6:  # Too many requests per second
                                # –õ–æ–≥–∏—Ä—É–µ–º rate limit —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö —Å–ª—É—á–∞–µ–≤
                                if self.rate_limit_hits < 10:
                                    self.log_warning(f"Rate limit –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}', –æ–∂–∏–¥–∞–Ω–∏–µ...")
                                self.rate_limit_hits += 1
                                await asyncio.sleep(1)
                                continue
                            else:
                                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ API —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
                                if self.requests_made < 100:
                                    self.log_error(f"–û—à–∏–±–∫–∞ VK API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {data['error']}")
                                return []
                        
                        if 'response' in data:
                            items = data['response'].get('items', [])
                            
                            # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
                            if self.config["enable_caching"]:
                                self.cache[cache_key] = {
                                    "data": items,
                                    "timestamp": time.time(),
                                    "query": query,
                                    "params": params
                                }
                                
                                # –£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                                self._smart_cache_cleanup()
                            
                            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
                            response_time = time.time() - start_time
                            self.response_times.append(response_time)
                            
                            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ rate limit –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω
                            if self.rate_limit_hits > 0:
                                self.rate_limit_hits = max(0, self.rate_limit_hits - 1)
                            
                            return items
                        else:
                            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
                            if self.requests_made < 50:
                                self.log_error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç VK API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {data}")
                            return []
                    else:
                        # –õ–æ–≥–∏—Ä—É–µ–º HTTP –æ—à–∏–±–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
                        if self.requests_made < 50:
                            self.log_error(f"HTTP –æ—à–∏–±–∫–∞ {response.status} –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
                        return []
                        
            except Exception as e:
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
                if self.requests_made < 100:
                    self.log_error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è '{query}': {e}")
                if attempt < retry_count - 1:
                    await asyncio.sleep(1)
                    continue
                return []
        
        return []

    def _parse_datetime(self, datetime_str: str) -> int:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –≤ timestamp
        """
        try:
            dt = datetime.strptime(datetime_str, "%d.%m.%Y %H:%M")
            return int(dt.timestamp())
        except ValueError as e:
            raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {datetime_str}") 

    async def mass_search_with_tokens(self, keyword_token_pairs: List[tuple], start_date, end_date, exact_match: bool = True, minus_words: List[str] = None, batch_size: int = None) -> List[Dict[str, Any]]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É–º–Ω–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π —Ç–æ–∫–µ–Ω–æ–≤
        """
        import time
        from datetime import datetime, timedelta, timezone
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π batch_size –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if batch_size is None:
            batch_size = self.config["batch_size"]
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
        total_queries = len(keyword_token_pairs)
        if total_queries > 10:
            self.log_info(f"üöÄ –ú–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫: {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤, batch_size={batch_size}")
        else:
            self.log_info(f"üöÄ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è {len(keyword_token_pairs)} –∑–∞–ø—Ä–æ—Å–æ–≤")
            self.log_info(f"‚öôÔ∏è Batch size: {batch_size}, Max batches: {self.config['max_batches']}")
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º start_date/end_date –∏–∑ –ú–°–ö –≤ UTC, –µ—Å–ª–∏ –æ–Ω–∏ —Å—Ç—Ä–æ–∫–∏
        def moscow_to_utc_timestamp(dt_str):
            moscow_tz = timezone(timedelta(hours=3))
            dt = datetime.strptime(dt_str, "%d.%m.%Y %H:%M")
            dt = dt.replace(tzinfo=moscow_tz)
            dt_utc = dt.astimezone(timezone.utc)
            return int(dt_utc.timestamp())
        
        _start_ts = start_date
        _end_ts = end_date
        if isinstance(start_date, str):
            _start_ts = moscow_to_utc_timestamp(start_date)
        if isinstance(end_date, str):
            _end_ts = moscow_to_utc_timestamp(end_date)
        
        all_posts = []
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ HTTP –∫–ª–∏–µ–Ω—Ç–∞
        timeout = aiohttp.ClientTimeout(total=self.config["timeout"])
        
        if self.config["use_connection_pooling"]:
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=20,
                ttl_dns_cache=300,
                use_dns_cache=True
            )
        else:
            connector = None
        
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–∞—Ç—á–∞–º–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–µ–π
            for i in range(0, len(keyword_token_pairs), batch_size):
                batch = keyword_token_pairs[i:i+batch_size]
                tasks = []
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
                if total_queries > 20 and i % (batch_size * 5) == 0:
                    progress = (i / len(keyword_token_pairs)) * 100
                    self.log_info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({i}/{total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤)")
                
                for keyword, token in batch:
                    # –£–º–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ - –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π
                    available_tokens = [t for _, t in keyword_token_pairs]
                    best_token = self._get_best_token(available_tokens)
                    
                    params = {
                        'q': f'"{keyword}"' if exact_match else keyword,
                        'count': 200,
                        'extended': 1,
                        'access_token': best_token,
                        'v': self.config["api_version"]
                    }
                    
                    if _start_ts is not None:
                        params['start_time'] = _start_ts
                    if _end_ts is not None:
                        params['end_time'] = _end_ts
                    
                    if minus_words:
                        for word in minus_words:
                            if word.strip():
                                params['q'] += f' -{word.strip()}'
                    
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Å—Ç–æ–≤
                    max_batches = self.config["max_batches"]
                    offsets = [j * 200 for j in range(max_batches)]
                    
                    for offset in offsets:
                        params_copy = params.copy()
                        params_copy['offset'] = offset
                        tasks.append(self._fetch_vk_batch(session, params_copy, keyword))
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, list):
                        all_posts.extend(result)
                    elif isinstance(result, Exception):
                        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –æ—à–∏–±–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
                        if total_queries <= 20 or self.requests_made < 50:
                            self.log_error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {result}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞—Ç—á–∞–º –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
                if total_queries > 20 and len(all_posts) > 0:
                    self.log_info(f"üìà –ë–∞—Ç—á {i//batch_size + 1}: –ø–æ–ª—É—á–µ–Ω–æ {len(all_posts)} –ø–æ—Å—Ç–æ–≤")
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–π –∫—ç—à
        self._cleanup_cache()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if total_queries > 10:
            self.log_info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {len(all_posts)} –ø–æ—Å—Ç–æ–≤, {self.requests_made} –∑–∞–ø—Ä–æ—Å–æ–≤")
        else:
            self.log_info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(all_posts)} –ø–æ—Å—Ç–æ–≤ –æ—Ç VK API")
            self.log_info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.requests_made} –∑–∞–ø—Ä–æ—Å–æ–≤, {len(self.response_times)} –∏–∑–º–µ—Ä–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–∏")
        
        return all_posts
    
    def _cleanup_cache(self):
        """–û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞"""
        if not self.config["enable_caching"]:
            return
        
        current_time = time.time()
        expired_keys = []
        
        for key, entry in self.cache.items():
            if not self._is_cache_valid(entry):
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            self.log_info(f"üßπ –û—á–∏—â–µ–Ω–æ {len(expired_keys)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞") 