"""
–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
"""

from datetime import datetime
from typing import Any, Dict, List

from src.core.event_system import EventType
from src.plugins.base_plugin import BasePlugin


class PostProcessorPlugin(BasePlugin):
    """–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π"""

    def __init__(self):
        super().__init__()
        self.name = "PostProcessorPlugin"
        self.version = "1.0.0"
        self.description = "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è + –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)"

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.config = {
            "enable_filtering": True,
            "enable_deduplication": True,
            "filter_method": "keywords",  # keywords, exact_match
            "deduplication_method": "link_hash",  # link_hash, text, content_hash
            "processing_order": ["deduplication", "filtering"],  # –ü–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            "batch_size": 1000,
            "enable_logging": True,
        }

        # –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –ø–ª–∞–≥–∏–Ω–∞–º–∏
        self.filter_plugin = None
        self.deduplication_plugin = None
        self.text_processing_plugin = None
        self.database_plugin = None

    def initialize(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        self.log_info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞ PostProcessor")

        if not self.validate_config():
            self.log_error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞")
            return

        self.log_info("–ü–ª–∞–≥–∏–Ω PostProcessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.emit_event(EventType.PLUGIN_LOADED, {"status": "initialized"})

    def shutdown(self) -> None:
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–ª–∞–≥–∏–Ω–∞"""
        self.log_info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–ª–∞–≥–∏–Ω–∞ PostProcessor")

        self.emit_event(EventType.PLUGIN_UNLOADED, {"status": "shutdown"})
        self.log_info("–ü–ª–∞–≥–∏–Ω PostProcessor –∑–∞–≤–µ—Ä—à–µ–Ω")

    def validate_config(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return True

    def get_required_config_keys(self) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return []

    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–ª–∞–≥–∏–Ω–∞"""
        return {
            "enabled": self.is_enabled(),
            "config": self.get_config(),
            "plugins_connected": {
                "filter": self.filter_plugin is not None,
                "deduplication": self.deduplication_plugin is not None,
                "text_processing": self.text_processing_plugin is not None,
                "database": self.database_plugin is not None,
            },
        }

    def set_filter_plugin(self, filter_plugin):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–≤—è–∑—å —Å –ø–ª–∞–≥–∏–Ω–æ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        self.filter_plugin = filter_plugin
        self.log_info("FilterPlugin –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ PostProcessorPlugin")

    def set_deduplication_plugin(self, deduplication_plugin):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–≤—è–∑—å —Å –ø–ª–∞–≥–∏–Ω–æ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        self.deduplication_plugin = deduplication_plugin
        self.log_info("DeduplicationPlugin –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ PostProcessorPlugin")

    def set_text_processing_plugin(self, text_processing_plugin):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–≤—è–∑—å —Å –ø–ª–∞–≥–∏–Ω–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        self.text_processing_plugin = text_processing_plugin
        self.log_info("TextProcessingPlugin –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ PostProcessorPlugin")

    def set_database_plugin(self, database_plugin):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–≤—è–∑—å —Å –ø–ª–∞–≥–∏–Ω–æ–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        self.database_plugin = database_plugin
        self.log_info("DatabasePlugin –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ PostProcessorPlugin")

    def process_posts(
        self,
        posts: List[Dict[str, Any]],
        keywords: List[str] = None,
        exact_match: bool = True,
        remove_duplicates: bool = True,
        processing_order: List[str] = None,
    ) -> Dict[str, Any]:
        """
        –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π

        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            exact_match: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            remove_duplicates: –£–¥–∞–ª—è—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã
            processing_order: –ü–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ['deduplication', 'filtering']

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        if not posts:
            return {
                "original_count": 0,
                "final_count": 0,
                "filtered_count": 0,
                "duplicates_removed": 0,
                "processing_time": 0,
                "final_posts": [],
            }

        start_time = datetime.now()
        original_count = len(posts)
        current_posts = posts.copy()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π: deduplication ‚Üí text_processing ‚Üí filtering)
        if processing_order is None:
            processing_order = self.config.get("processing_order", ["deduplication", "text_processing", "filtering"])

        self.log_info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {original_count} –ø—É–±–ª–∏–∫–∞—Ü–∏–π")
        self.log_info(f"üìã –ü–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_order}")

        # –≠—Ç–∞–ø 1: –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicates_removed = 0
        if remove_duplicates and "deduplication" in processing_order:
            if self.deduplication_plugin:
                before_count = len(current_posts)
                current_posts = self.deduplication_plugin.remove_duplicates_by_link_hash(current_posts)
                duplicates_removed = before_count - len(current_posts)
                self.log_info(f"üóëÔ∏è –≠—Ç–∞–ø 1: –£–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
            else:
                self.log_warning("DeduplicationPlugin –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")

        # –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
        text_processed = 0
        if keywords and "text_processing" in processing_order:
            if self.text_processing_plugin:
                before_count = len(current_posts)
                current_posts = self.text_processing_plugin.process_posts_text(current_posts)
                text_processed = before_count  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
                self.log_info(f"üìù –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {text_processed} —Ç–µ–∫—Å—Ç–æ–≤")
            else:
                self.log_warning("TextProcessingPlugin –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")

        # –≠—Ç–∞–ø 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        filtered_count = 0
        if keywords and "filtering" in processing_order:
            if self.filter_plugin:
                before_count = len(current_posts)
                current_posts = self.filter_plugin.filter_posts_by_multiple_keywords(
                    current_posts, keywords, exact_match
                )
                filtered_count = before_count - len(current_posts)
                self.log_info(f"üîç –≠—Ç–∞–ø 3: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} –ø—É–±–ª–∏–∫–∞—Ü–∏–π")
            else:
                self.log_warning("FilterPlugin –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")

        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_time = (datetime.now() - start_time).total_seconds()

        result = {
            "original_count": original_count,
            "final_count": len(current_posts),
            "filtered_count": filtered_count,
            "duplicates_removed": duplicates_removed,
            "text_processed": text_processed,
            "processing_time": processing_time,
            "final_posts": current_posts,
            "processing_order": processing_order,
        }

        self.log_info(
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {original_count} ‚Üí {len(current_posts)} "
            f"(–¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed}, —Ç–µ–∫—Å—Ç–æ–≤: {text_processed}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {filtered_count})"
        )

        return result

    def process_posts_from_database(
        self, task_id: int, keywords: List[str] = None, exact_match: bool = True, remove_duplicates: bool = True
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

        Args:
            task_id: ID –∑–∞–¥–∞—á–∏
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            exact_match: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            remove_duplicates: –£–¥–∞–ª—è—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        if not self.database_plugin:
            self.log_error("DatabasePlugin –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")
            return {}

        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ –ë–î
            posts = self.database_plugin.get_task_posts(task_id)

            if not posts:
                self.log_info(f"–ó–∞–¥–∞—á–∞ {task_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—É–±–ª–∏–∫–∞—Ü–∏–π")
                return {"task_id": task_id, "posts_count": 0}

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            result = self.process_posts(posts, keywords, exact_match, remove_duplicates)
            result["task_id"] = task_id

            return result

        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∏–∑ –ë–î: {e}")
            return {}

    def clean_database_task(self, task_id: int, keywords: List[str] = None, exact_match: bool = True) -> Dict[str, int]:
        """
        –û—á–∏—Å—Ç–∫–∞ –∑–∞–¥–∞—á–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (—É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ + –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö)

        Args:
            task_id: ID –∑–∞–¥–∞—á–∏
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            exact_match: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—á–∏—Å—Ç–∫–∏
        """
        if not self.database_plugin:
            self.log_error("DatabasePlugin –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")
            return {}

        results = {"duplicates_removed": 0, "invalid_posts_removed": 0, "total_cleaned": 0}

        try:
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if self.deduplication_plugin:
                results["duplicates_removed"] = self.deduplication_plugin.clean_duplicates_from_database(task_id)

            # –£–¥–∞–ª—è–µ–º –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            if keywords and self.filter_plugin:
                results["invalid_posts_removed"] = self.filter_plugin.clean_by_parsing_parameters(
                    task_id, keywords, exact_match
                )

            results["total_cleaned"] = results["duplicates_removed"] + results["invalid_posts_removed"]

            self.log_info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –∑–∞–¥–∞—á–∏ {task_id}: {results}")
            return results

        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∑–∞–¥–∞—á–∏: {e}")
            return results

    def get_processing_statistics(self, task_id: int = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏

        Args:
            task_id: ID –∑–∞–¥–∞—á–∏ (None –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        stats = {
            "filter_plugin_connected": self.filter_plugin is not None,
            "deduplication_plugin_connected": self.deduplication_plugin is not None,
            "database_plugin_connected": self.database_plugin is not None,
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if self.deduplication_plugin:
            stats["duplicates"] = self.deduplication_plugin.get_database_duplicate_statistics(task_id)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        if self.filter_plugin:
            stats["filtering"] = self.filter_plugin.get_statistics()

        return stats

    # === –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ú–ï–¢–û–î–´ –û–ë–†–ê–ë–û–¢–ö–ò ===

    def process_posts_optimized(
        self,
        posts: List[Dict],
        keywords: List[str] = None,
        exact_match: bool = True,
        early_termination: bool = True,
        lazy_processing: bool = True
    ) -> Dict[str, Any]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ —Ä–∞–Ω–Ω–∏–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º

        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            exact_match: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            early_termination: –†–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
            lazy_processing: –õ–µ–Ω–∏–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        """
        start_time = datetime.now()
        self.log_info(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(posts)} –ø—É–±–ª–∏–∫–∞—Ü–∏–π")

        current_posts = posts.copy()
        original_count = len(posts)
        stats = {
            "original_count": original_count,
            "duplicates_removed": 0,
            "text_processed": 0,
            "filtered_count": 0,
            "early_exit": False,
            "lazy_skips": 0
        }

        # –≠—Ç–∞–ø 1: –ë—ã—Å—Ç—Ä–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ö—ç—à–∞–º —Å—Å—ã–ª–æ–∫
        if self.config.get("enable_deduplication", True):
            before_count = len(current_posts)
            seen_links = set()
            unique_posts = []

            for post in current_posts:
                link_hash = post.get('link_hash', post.get('link', ''))
                if link_hash not in seen_links:
                    seen_links.add(link_hash)
                    unique_posts.append(post)

                # –†–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥ –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ —Ä–∞–∑—É–º–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞
                if early_termination and len(unique_posts) >= 5000:
                    stats["early_exit"] = True
                    self.log_info("‚ö° –†–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç 5000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤")
                    break

            current_posts = unique_posts
            stats["duplicates_removed"] = before_count - len(current_posts)
            self.log_info(f"üîó –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: —É–¥–∞–ª–µ–Ω–æ {stats['duplicates_removed']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

        # –≠—Ç–∞–ø 2: –õ–µ–Ω–∏–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
        if keywords and lazy_processing:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–π–¥—É—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
            if self.text_processing_plugin:
                processed_posts = []
                for post in current_posts:
                    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                    text = post.get('text', '').lower()
                    has_keywords = any(kw.lower() in text for kw in keywords[:3])  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3

                    if has_keywords or not exact_match:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ —É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø–æ—Å—Ç–æ–≤
                        processed_text = self.text_processing_plugin.clean_text_completely(post.get('text', ''))
                        post['cleaned_text'] = processed_text
                        processed_posts.append(post)
                        stats["text_processed"] += 1
                    else:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–≤–µ–¥–æ–º–æ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø–æ—Å—Ç–æ–≤
                        stats["lazy_skips"] += 1

                current_posts = processed_posts
                self.log_info(f"üìù –õ–µ–Ω–∏–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {stats['text_processed']} —Ç–µ–∫—Å—Ç–æ–≤, –ø—Ä–æ–ø—É—â–µ–Ω–æ {stats['lazy_skips']}")

        # –≠—Ç–∞–ø 3: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        if keywords and self.filter_plugin:
            before_count = len(current_posts)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º cleaned_text –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ –æ–±—ã—á–Ω—ã–π text
            for post in current_posts:
                if 'cleaned_text' not in post and self.text_processing_plugin:
                    post['cleaned_text'] = self.text_processing_plugin.clean_text_completely(post.get('text', ''))

            current_posts = self.filter_plugin.filter_posts_by_multiple_keywords(
                current_posts, keywords, exact_match
            )
            stats["filtered_count"] = before_count - len(current_posts)
            self.log_info(f"üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {stats['filtered_count']} –ø—É–±–ª–∏–∫–∞—Ü–∏–π")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        processing_time = (datetime.now() - start_time).total_seconds()
        result = {
            **stats,
            "final_count": len(current_posts),
            "processing_time": processing_time,
            "final_posts": current_posts,
            "optimization_level": "high"
        }

        self.log_info(
            f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å: "
            f"{original_count} ‚Üí {len(current_posts)} (—ç–∫–æ–Ω–æ–º: {stats['lazy_skips']} –ø—Ä–æ–ø—É—Å–∫–æ–≤)"
        )

        return result

    def process_posts_in_batches(
        self,
        posts: List[Dict],
        keywords: List[str] = None,
        exact_match: bool = True,
        batch_size: int = None
    ) -> Dict[str, Any]:
        """
        –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö

        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            exact_match: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
        """
        start_time = datetime.now()
        if batch_size is None:
            batch_size = self.config.get("batch_size", 1000)

        total_posts = len(posts)
        self.log_info(f"üì¶ –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_posts} –ø—É–±–ª–∏–∫–∞—Ü–∏–π (–±–∞—Ç—á: {batch_size})")

        all_results = []
        total_stats = {
            "original_count": total_posts,
            "batches_processed": 0,
            "duplicates_removed": 0,
            "text_processed": 0,
            "filtered_count": 0
        }

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
        for i in range(0, total_posts, batch_size):
            batch = posts[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (total_posts + batch_size - 1) // batch_size

            self.log_info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch)} –ø–æ—Å—Ç–æ–≤)")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á
            batch_result = self.process_posts_optimized(
                batch, keywords, exact_match,
                early_termination=False,  # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –±–∞—Ç—á–∏ –¥–æ—Å—Ä–æ—á–Ω–æ
                lazy_processing=True
            )

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_results.extend(batch_result["final_posts"])
            total_stats["duplicates_removed"] += batch_result["duplicates_removed"]
            total_stats["text_processed"] += batch_result["text_processed"]
            total_stats["filtered_count"] += batch_result["filtered_count"]
            total_stats["batches_processed"] += 1

            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            if batch_num % 5 == 0 or batch_num == total_batches:
                progress = (batch_num / total_batches) * 100
                self.log_info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({batch_num}/{total_batches} –±–∞—Ç—á–µ–π)")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
        self.log_info("üîó –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏...")
        seen_links = set()
        final_unique = []
        for post in all_results:
            link_hash = post.get('link_hash', post.get('link', ''))
            if link_hash not in seen_links:
                seen_links.add(link_hash)
                final_unique.append(post)

        cross_batch_duplicates = len(all_results) - len(final_unique)
        total_stats["duplicates_removed"] += cross_batch_duplicates

        processing_time = (datetime.now() - start_time).total_seconds()
        result = {
            **total_stats,
            "final_count": len(final_unique),
            "cross_batch_duplicates": cross_batch_duplicates,
            "processing_time": processing_time,
            "batch_size": batch_size,
            "final_posts": final_unique,
            "optimization_level": "batch"
        }

        self.log_info(
            f"‚úÖ –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å: "
            f"{total_posts} ‚Üí {len(final_unique)} ({total_stats['batches_processed']} –±–∞—Ç—á–µ–π)"
        )

        return result

    def process_posts_with_cache(
        self,
        posts: List[Dict],
        keywords: List[str] = None,
        exact_match: bool = True,
        cache_results: bool = True
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            exact_match: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            cache_results: –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        start_time = datetime.now()
        self.log_info(f"üíæ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º {len(posts)} –ø—É–±–ª–∏–∫–∞—Ü–∏–π")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞ (–ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ –ø–∞–º—è—Ç–∏)
        if not hasattr(self, '_processing_cache'):
            self._processing_cache = {
                'text_processing': {},  # hash -> cleaned_text
                'filtering': {},        # (text_hash, keywords_hash) -> bool
                'duplicates': set()     # link_hashes
            }

        current_posts = posts.copy()
        original_count = len(posts)
        cache_hits = {"text": 0, "filter": 0, "dedup": 0}

        # –≠—Ç–∞–ø 1: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å –∫—ç—à–µ–º
        if self.config.get("enable_deduplication", True):
            before_count = len(current_posts)
            unique_posts = []

            for post in current_posts:
                link_hash = post.get('link_hash', post.get('link', ''))

                if link_hash in self._processing_cache['duplicates']:
                    cache_hits["dedup"] += 1
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç
                else:
                    self._processing_cache['duplicates'].add(link_hash)
                    unique_posts.append(post)

            current_posts = unique_posts
            duplicates_removed = before_count - len(current_posts)
            self.log_info(f"üîó –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: —É–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–∫—ç—à: {cache_hits['dedup']})")

        # –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∫—ç—à–µ–º
        text_processed = 0
        if keywords and self.text_processing_plugin:
            for post in current_posts:
                text = post.get('text', '')
                text_hash = str(hash(text))

                if text_hash in self._processing_cache['text_processing']:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    post['cleaned_text'] = self._processing_cache['text_processing'][text_hash]
                    cache_hits["text"] += 1
                else:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –∫—ç—à–∏—Ä—É–µ–º
                    cleaned_text = self.text_processing_plugin.clean_text_completely(text)
                    post['cleaned_text'] = cleaned_text
                    if cache_results:
                        self._processing_cache['text_processing'][text_hash] = cleaned_text

                text_processed += 1

            self.log_info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: {text_processed} —Ç–µ–∫—Å—Ç–æ–≤ (–∫—ç—à: {cache_hits['text']})")

        # –≠—Ç–∞–ø 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –∫—ç—à–µ–º
        filtered_count = 0
        if keywords and self.filter_plugin:
            before_count = len(current_posts)
            keywords_hash = str(hash(tuple(sorted(keywords)) + (exact_match,)))

            filtered_posts = []
            for post in current_posts:
                text_to_check = post.get('cleaned_text', post.get('text', ''))
                text_hash = str(hash(text_to_check))
                cache_key = (text_hash, keywords_hash)

                if cache_key in self._processing_cache['filtering']:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    if self._processing_cache['filtering'][cache_key]:
                        filtered_posts.append(post)
                    cache_hits["filter"] += 1
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∫—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞
                    single_post_result = self.filter_plugin.filter_posts_by_multiple_keywords(
                        [post], keywords, exact_match
                    )
                    matches = len(single_post_result) > 0

                    if cache_results:
                        self._processing_cache['filtering'][cache_key] = matches

                    if matches:
                        filtered_posts.append(post)

            current_posts = filtered_posts
            filtered_count = before_count - len(current_posts)
            self.log_info(f"üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} –ø—É–±–ª–∏–∫–∞—Ü–∏–π (–∫—ç—à: {cache_hits['filter']})")

        processing_time = (datetime.now() - start_time).total_seconds()
        cache_efficiency = sum(cache_hits.values()) / max(original_count, 1) * 100

        result = {
            "original_count": original_count,
            "final_count": len(current_posts),
            "filtered_count": filtered_count,
            "duplicates_removed": duplicates_removed,
            "text_processed": text_processed,
            "cache_hits": cache_hits,
            "cache_efficiency": cache_efficiency,
            "processing_time": processing_time,
            "final_posts": current_posts,
            "optimization_level": "cached"
        }

        self.log_info(
            f"‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å: "
            f"{original_count} ‚Üí {len(current_posts)} (–∫—ç—à: {cache_efficiency:.1f}%)"
        )

        return result

    def process_posts_parallel(
        self,
        posts: List[Dict],
        keywords: List[str] = None,
        exact_match: bool = True,
        max_workers: int = None
    ) -> Dict[str, Any]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ThreadPoolExecutor

        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            exact_match: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        """
        start_time = datetime.now()

        try:
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import threading
        except ImportError:
            self.log_warning("concurrent.futures –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
            return self.process_posts_optimized(posts, keywords, exact_match)

        if max_workers is None:
            max_workers = min(4, len(posts) // 1000 + 1)  # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤

        total_posts = len(posts)
        self.log_info(f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_posts} –ø—É–±–ª–∏–∫–∞—Ü–∏–π ({max_workers} –ø–æ—Ç–æ–∫–æ–≤)")

        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        chunk_size = max(100, total_posts // max_workers)
        chunks = [posts[i:i + chunk_size] for i in range(0, total_posts, chunk_size)]

        all_results = []
        total_stats = {
            "original_count": total_posts,
            "threads_used": len(chunks),
            "duplicates_removed": 0,
            "text_processed": 0,
            "filtered_count": 0
        }

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏
            future_to_chunk = {}
            for i, chunk in enumerate(chunks):
                future = executor.submit(
                    self._process_chunk_thread_safe,
                    chunk, keywords, exact_match, i + 1
                )
                future_to_chunk[future] = i

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            completed_chunks = 0
            for future in as_completed(future_to_chunk):
                chunk_idx = future_to_chunk[future]
                try:
                    chunk_result = future.result()
                    all_results.extend(chunk_result["final_posts"])
                    total_stats["duplicates_removed"] += chunk_result["duplicates_removed"]
                    total_stats["text_processed"] += chunk_result["text_processed"]
                    total_stats["filtered_count"] += chunk_result["filtered_count"]

                    completed_chunks += 1
                    self.log_info(f"‚úÖ –ß–∞–Ω–∫ {chunk_idx + 1}/{len(chunks)} –∑–∞–≤–µ—Ä—à—ë–Ω ({completed_chunks}/{len(chunks)})")

                except Exception as e:
                    self.log_error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞–Ω–∫–µ {chunk_idx + 1}: {e}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        self.log_info("üîó –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É –ø–æ—Ç–æ–∫–∞–º–∏...")
        seen_links = set()
        final_unique = []
        for post in all_results:
            link_hash = post.get('link_hash', post.get('link', ''))
            if link_hash not in seen_links:
                seen_links.add(link_hash)
                final_unique.append(post)

        cross_thread_duplicates = len(all_results) - len(final_unique)
        total_stats["duplicates_removed"] += cross_thread_duplicates

        processing_time = (datetime.now() - start_time).total_seconds()
        result = {
            **total_stats,
            "final_count": len(final_unique),
            "cross_thread_duplicates": cross_thread_duplicates,
            "processing_time": processing_time,
            "max_workers": max_workers,
            "chunk_size": chunk_size,
            "final_posts": final_unique,
            "optimization_level": "parallel"
        }

        self.log_info(
            f"‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å: "
            f"{total_posts} ‚Üí {len(final_unique)} ({max_workers} –ø–æ—Ç–æ–∫–æ–≤)"
        )

        return result

    def _process_chunk_thread_safe(
        self,
        chunk: List[Dict],
        keywords: List[str],
        exact_match: bool,
        chunk_id: int
    ) -> Dict[str, Any]:
        """
        Thread-safe –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        return self.process_posts_optimized(
            chunk,
            keywords=keywords,
            exact_match=exact_match,
            early_termination=False,  # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ—Å—Ä–æ—á–Ω–æ
            lazy_processing=True
        )

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        if hasattr(self, '_processing_cache'):
            self._processing_cache = {
                'text_processing': {},
                'filtering': {},
                'duplicates': set()
            }
            self.log_info("üíæ –ö—ç—à –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–∏—â–µ–Ω")

    def get_cache_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        if not hasattr(self, '_processing_cache'):
            return {"cache_enabled": False}

        cache = self._processing_cache
        return {
            "cache_enabled": True,
            "text_cache_size": len(cache['text_processing']),
            "filter_cache_size": len(cache['filtering']),
            "duplicates_cache_size": len(cache['duplicates']),
            "total_memory_items": (
                len(cache['text_processing']) +
                len(cache['filtering']) +
                len(cache['duplicates'])
            )
        }

    # === –ö–û–ù–ï–¶ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–• –ú–ï–¢–û–î–û–í ===
