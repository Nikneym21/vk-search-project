# üîç VKSearchPlugin

## üìã **–û–ü–ò–°–ê–ù–ò–ï**

**VKSearchPlugin** - –ø–ª–∞–≥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π –≤ –í–ö–æ–Ω—Ç–∞–∫—Ç–µ —á–µ—Ä–µ–∑ VK API. –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω–∞–º–∏ —á–µ—Ä–µ–∑ TokenManagerPlugin.

## üéØ **–û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨**

- –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ VK API
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–º–∏ —á–µ—Ä–µ–∑ TokenManagerPlugin
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –ª–∏–º–∏—Ç–æ–≤ API
- –ü–æ–¥—á–∏–Ω—è–µ—Ç—Å—è PluginManager

## üîß **–û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´**

```python
def mass_search_with_tokens(self, keyword_token_pairs: List[Tuple[str, str]], 
                          start_date: str, end_date: str, exact_match: bool = True,
                          minus_words: List[str] = None) -> List[Dict]:
    """–ú–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–æ–≤"""

def search_with_single_token(self, keyword: str, token: str, start_date: str,
                           end_date: str, exact_match: bool = True) -> List[Dict]:
    """–ü–æ–∏—Å–∫ —Å –æ–¥–Ω–∏–º —Ç–æ–∫–µ–Ω–æ–º"""

def get_statistics(self) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–∞"""
```

## ‚ö° **–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò**

### **1. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –û–±—Ä–∞–±–æ—Ç–∫–∞ –ó–∞–ø—Ä–æ—Å–æ–≤**
```python
async def mass_search_parallel(self, keyword_token_pairs: List[Tuple[str, str]], 
                             start_date: str, end_date: str, exact_match: bool = True,
                             minus_words: List[str] = None, max_concurrent=5) -> List[Dict]:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def search_with_semaphore(keyword: str, token: str):
        async with semaphore:
            return await self._search_async(keyword, token, start_date, end_date, exact_match, minus_words)
    
    tasks = []
    for keyword, token in keyword_token_pairs:
        task = search_with_semaphore(keyword, token)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    all_posts = []
    for result in results:
        if isinstance(result, list):
            all_posts.extend(result)
        else:
            self.log_error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {result}")
    
    return all_posts
```

### **2. –£–º–Ω–æ–µ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**
```python
def search_with_cache(self, keyword: str, token: str, start_date: str, end_date: str,
                     exact_match: bool = True, cache_ttl=3600) -> List[Dict]:
    """–ü–æ–∏—Å–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    cache_key = self._generate_cache_key(keyword, token, start_date, end_date, exact_match)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
    if cache_key in self.search_cache:
        cache_entry = self.search_cache[cache_key]
        if time.time() - cache_entry['timestamp'] < cache_ttl:
            self.log_info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ –¥–ª—è –∫–ª—é—á–∞: {keyword}")
            return cache_entry['data']
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
    result = self.search_with_single_token(keyword, token, start_date, end_date, exact_match)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
    self.search_cache[cache_key] = {
        'data': result,
        'timestamp': time.time()
    }
    
    return result
```

### **3. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –†–æ—Ç–∞—Ü–∏—è –¢–æ–∫–µ–Ω–æ–≤**
```python
def adaptive_token_rotation(self, keyword_token_pairs: List[Tuple[str, str]]) -> List[Tuple[str, str]]:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤
    token_performance = {}
    for token in set(token for _, token in keyword_token_pairs):
        performance = self._analyze_token_performance(token)
        token_performance[token] = performance
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    sorted_tokens = sorted(token_performance.items(), key=lambda x: x[1], reverse=True)
    
    # –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    optimized_pairs = []
    token_index = 0
    
    for keyword, _ in keyword_token_pairs:
        if token_index >= len(sorted_tokens):
            token_index = 0
        
        optimized_pairs.append((keyword, sorted_tokens[token_index][0]))
        token_index += 1
    
    return optimized_pairs
```

### **4. –ë–∞—Ç—á–µ–≤–∞—è –û–±—Ä–∞–±–æ—Ç–∫–∞ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**
```python
def process_results_in_batches(self, raw_results: List[Dict], batch_size=100) -> List[Dict]:
    """–ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
    
    processed_results = []
    
    for i in range(0, len(raw_results), batch_size):
        batch = raw_results[i:i + batch_size]
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
        processed_batch = self._process_batch_parallel(batch)
        processed_results.extend(processed_batch)
        
        self.log_info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_results)} –∏–∑ {len(raw_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    return processed_results

def _process_batch_parallel(self, batch: List[Dict]) -> List[Dict]:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    def process_single_result(result: Dict) -> Dict:
        # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        processed = {
            'vk_id': result.get('id'),
            'title': result.get('title', ''),
            'content': self._clean_content(result.get('text', '')),
            'author_id': result.get('owner_id'),
            'author_name': result.get('owner_name', ''),
            'likes': result.get('likes', {}).get('count', 0),
            'comments': result.get('comments', {}).get('count', 0),
            'reposts': result.get('reposts', {}).get('count', 0),
            'views': result.get('views', {}).get('count', 0),
            'link': f"https://vk.com/wall{result.get('owner_id')}_{result.get('id')}",
            'created_date': result.get('date')
        }
        return processed
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        processed_batch = list(executor.map(process_single_result, batch))
    
    return processed_batch
```

### **5. –£–º–Ω–∞—è –û–±—Ä–∞–±–æ—Ç–∫–∞ –û—à–∏–±–æ–∫**
```python
def smart_error_handling(self, keyword: str, token: str, max_retries=3):
    """–£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏"""
    
    for attempt in range(max_retries):
        try:
            result = self._make_api_request(keyword, token)
            return result
            
        except Exception as e:
            error_type = type(e).__name__
            
            if error_type == 'RateLimitError':
                # –û–∂–∏–¥–∞–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–æ–≤
                wait_time = self._calculate_wait_time(attempt)
                self.log_warning(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç API, –æ–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫—É–Ω–¥")
                time.sleep(wait_time)
                
            elif error_type == 'TokenExpiredError':
                # –ó–∞–º–µ–Ω–∞ —Ç–æ–∫–µ–Ω–∞
                new_token = self.token_manager.get_next_token()
                self.log_info(f"–¢–æ–∫–µ–Ω –∏—Å—Ç–µ–∫, –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ –Ω–æ–≤—ã–π")
                token = new_token
                
            elif error_type == 'NetworkError':
                # –ü–æ–≤—Ç–æ—Ä –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö
                wait_time = 2 ** attempt
                self.log_warning(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait_time} —Å–µ–∫—É–Ω–¥")
                time.sleep(wait_time)
                
            else:
                # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
                self.log_error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
                break
    
    return []
```

### **6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ó–∞–ø—Ä–æ—Å–æ–≤**
```python
def optimize_search_queries(self, keywords: List[str]) -> List[str]:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    optimized_keywords = []
    
    for keyword in keywords:
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        cleaned = ' '.join(keyword.split())
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞–≤—ã—á–µ–∫ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if ' ' in cleaned:
            optimized = f'"{cleaned}"'
        else:
            optimized = cleaned
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if len(cleaned) >= 3:
            optimized_keywords.append(optimized)
    
    return optimized_keywords
```

## üìä **–ú–ï–¢–†–ò–ö–ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò**

### **–û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
- **–°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞**: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤ 3-5 —Ä–∞–∑ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤**: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤ 2-3 —Ä–∞–∑–∞ (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è)
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: 70-80% –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫—ç—à–∞
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫**: —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ 50%
- **–ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞**: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –≤ 2-3 —Ä–∞–∑–∞

## üîó **–ó–ê–í–ò–°–ò–ú–û–°–¢–ò**

- **PluginManager**: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–ª–∞–≥–∏–Ω–æ–º
- **TokenManagerPlugin**: –ø–æ–ª—É—á–µ–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–º–∏
- **DatabasePlugin**: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞

## üß™ **–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï**

```python
def test_search_performance():
    """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞"""
    
    keywords = ['—Ç–µ—Å—Ç', '–Ω–æ–≤–æ—Å—Ç–∏', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏']
    tokens = ['token1', 'token2', 'token3']
    
    # –¢–µ—Å—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    start_time = time.time()
    result1 = vk_plugin.mass_search_parallel(keyword_token_pairs, start_date, end_date)
    time1 = time.time() - start_time
    
    # –¢–µ—Å—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    start_time = time.time()
    result2 = vk_plugin.mass_search_with_tokens(keyword_token_pairs, start_date, end_date)
    time2 = time.time() - start_time
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±—ã—Å—Ç—Ä–µ–µ
    assert time1 < time2
```

## üìà **–ü–õ–ê–ù –†–ê–ó–í–ò–¢–ò–Ø**

1. **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤**
2. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**
3. **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Ä–æ—Ç–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤**
4. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**
5. **–£–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫**
6. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤**

---

**–í–µ—Ä—Å–∏—è:** 1.0  
**–°—Ç–∞—Ç—É—Å:** –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ 