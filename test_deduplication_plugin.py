#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –ø–ª–∞–≥–∏–Ω–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import sys
import os
import json
import pandas as pd
import asyncio

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from plugins.deduplication.deduplication_plugin import DeduplicationPlugin

def load_csv_as_posts(csv_file):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV —Ñ–∞–π–ª –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –ø–æ—Å—Ç–æ–≤"""
    print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º {csv_file}...")
    
    try:
        df = pd.read_csv(csv_file)
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –ø–æ—Å—Ç–æ–≤
        posts = []
        for _, row in df.iterrows():
            post = {
                'id': row.get('id', 0),
                'owner_id': row.get('owner_id', 0),
                'text': row.get('text', ''),
                'date': row.get('date', 0),
                'likes': row.get('likes', 0),
                'comments': row.get('comments', 0),
                'reposts': row.get('reposts', 0),
                'views': row.get('views', 0)
            }
            posts.append(post)
        
        print(f"   –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ {len(posts)} –ø–æ—Å—Ç–æ–≤")
        return posts
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {csv_file}: {e}")
        return []

def analyze_duplicates_manual(csv_file):
    """–†—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ CSV —Ñ–∞–π–ª–µ"""
    print(f"\nüîç –†–£–ß–ù–û–ô –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í: {csv_file}")
    print("="*60)
    
    try:
        df = pd.read_csv(csv_file)
        print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ä–∞–∑–Ω—ã–º –ø–æ–ª—è–º
        print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ (owner_id, id)
        df['post_key'] = df['owner_id'].astype(str) + '_' + df['id'].astype(str)
        duplicate_keys = df[df.duplicated(subset=['post_key'], keep=False)]
        print(f"   –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ (owner_id, id): {len(duplicate_keys)} —Å—Ç—Ä–æ–∫")
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
        text_duplicates = df[df.duplicated(subset=['text'], keep=False)]
        print(f"   –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É: {len(text_duplicates)} —Å—Ç—Ä–æ–∫")
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ—Å—Ç—ã
        unique_posts = df.drop_duplicates(subset=['post_key'])
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤: {len(unique_posts)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if len(duplicate_keys) > 0:
            print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
            duplicate_groups = df.groupby('post_key').filter(lambda x: len(x) > 1)
            for i, (key, group) in enumerate(duplicate_groups.groupby('post_key')[:3]):
                print(f"   –ì—Ä—É–ø–ø–∞ {i+1} (–∫–ª—é—á: {key}): {len(group)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
                for j, row in group.iterrows():
                    text = row['text'][:50] + "..." if len(row['text']) > 50 else row['text']
                    print(f"     {j}: {text}")
        
        return {
            'total': len(df),
            'duplicate_keys': len(duplicate_keys),
            'text_duplicates': len(text_duplicates),
            'unique': len(unique_posts)
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {csv_file}: {e}")
        return None

async def test_deduplication_method(csv_file, dedup_plugin, method):
    """–¢–µ—Å—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
    print(f"\nüß™ –¢–ï–°–¢ –ú–ï–¢–û–î–ê: {method.upper()}")
    print("="*40)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã
    posts = load_csv_as_posts(csv_file)
    if not posts:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å—Ç—ã")
        return None
    
    print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(f"   –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
    try:
        if method == "post_id":
            unique_posts = dedup_plugin.remove_duplicates_by_post_id(posts)
        elif method == "text":
            unique_posts = dedup_plugin.remove_duplicates_by_text(posts)
        elif method == "content_hash":
            unique_posts = dedup_plugin.remove_duplicates_by_content_hash(posts)
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")
            return None
        
        removed_count = len(posts) - len(unique_posts)
        
        print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤: {len(posts)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤: {len(unique_posts)}")
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed_count}")
        print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏: {(len(unique_posts) / len(posts) * 100):.1f}%")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
        if unique_posts:
            print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤:")
            for i, post in enumerate(unique_posts[:3]):
                text = post.get('text', '')[:100]
                owner_id = post.get('owner_id', 0)
                post_id = post.get('id', 0)
                print(f"   {i+1}. [ID: {owner_id}_{post_id}] {text}...")
        
        return {
            'method': method,
            'total': len(posts),
            'unique': len(unique_posts),
            'removed': removed_count,
            'uniqueness_percent': (len(unique_posts) / len(posts) * 100)
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return None

async def test_parallel_deduplication(csv_file, dedup_plugin, method):
    """–¢–µ—Å—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
    print(f"\nüöÄ –¢–ï–°–¢ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û–ô –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò: {method.upper()}")
    print("="*50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã
    posts = load_csv_as_posts(csv_file)
    if not posts:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å—Ç—ã")
        return None
    
    print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(f"   –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
    try:
        unique_posts = await dedup_plugin.remove_duplicates_parallel(posts, method=method)
        
        removed_count = len(posts) - len(unique_posts)
        
        print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤: {len(posts)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤: {len(unique_posts)}")
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed_count}")
        print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏: {(len(unique_posts) / len(posts) * 100):.1f}%")
        
        return {
            'method': f"parallel_{method}",
            'total': len(posts),
            'unique': len(unique_posts),
            'removed': removed_count,
            'uniqueness_percent': (len(unique_posts) / len(posts) * 100)
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return None

async def test_deduplication_plugin():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç –ø–ª–∞–≥–∏–Ω–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
    print("üß™ –¢–µ—Å—Ç –ø–ª–∞–≥–∏–Ω–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏")
    
    # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–≥–∏–Ω
    dedup_plugin = DeduplicationPlugin()
    dedup_plugin.initialize()
    
    # –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    csv_files = [
        'data/results/search_20250726_045350.csv',
        'data/results/search_20250726_051718.csv'
    ]
    
    all_results = []
    
    for csv_file in csv_files:
        print(f"\n{'='*60}")
        print(f"üìã –¢–ï–°–¢ –§–ê–ô–õ–ê: {csv_file}")
        print(f"{'='*60}")
        
        # –†—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        manual_stats = analyze_duplicates_manual(csv_file)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        methods = ["post_id", "text", "content_hash"]
        file_results = []
        
        for method in methods:
            result = await test_deduplication_method(csv_file, dedup_plugin, method)
            if result:
                file_results.append(result)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
        parallel_result = await test_parallel_deduplication(csv_file, dedup_plugin, "post_id")
        if parallel_result:
            file_results.append(parallel_result)
        
        # –°–≤–æ–¥–∫–∞ –ø–æ —Ñ–∞–π–ª—É
        if file_results:
            print(f"\nüìä –°–í–û–î–ö–ê –ü–û –§–ê–ô–õ–£:")
            for result in file_results:
                print(f"   {result['method']}: {result['total']} -> {result['unique']} "
                      f"({result['uniqueness_percent']:.1f}% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏)")
        
        all_results.extend(file_results)
    
    # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
    print(f"\n{'='*60}")
    print(f"üìä –û–ë–©–ê–Ø –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print(f"{'='*60}")
    
    for result in all_results:
        print(f"üìÅ {result['method']}:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö: {result['total']} –ø–æ—Å—Ç–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {result['unique']} –ø–æ—Å—Ç–æ–≤")
        print(f"   –£–¥–∞–ª–µ–Ω–æ: {result['removed']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {result['uniqueness_percent']:.1f}%")
        print()
    
    # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É
    dedup_plugin.shutdown()
    print(f"‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == "__main__":
    asyncio.run(test_deduplication_plugin()) 